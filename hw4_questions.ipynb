{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "707a2c40",
   "metadata": {},
   "source": [
    "## HW 4 - Domain Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba5dc26",
   "metadata": {},
   "source": [
    "### Section 1 - Classification\n",
    "[1] (a) **Implement a \"Gradient Reversal Layer\"**. Refer to the paper by Ganin et al. [3]. The main idea is that we want the feature extractor to be domain invariant, which can be enforced by reversing the gradient characteristics emerging from a \"Domain Classifer\" or an \"Adversarial Network\". There are multiple ways to do this. One way is to define a \"hook\" (refer to PyTorch docs) attached to the input of the adversial network which is tasked with inverting the gradient direction. Another way is to define a network module (Check `AdversarialLayer` in `classification/network.py`), with the forward function as the identity and the backward function reversing the gradients. Formally, you have to define a function GRL(`grad`) = -1 * `grad`, where `grad` corresponds to the gradient $\\frac{\\partial y}{\\partial x}$. Here, $y$ is the output of the adversarial network and $x$ is the input. Look out for \"IMPLEMENT YOUR CODE HERE\" in the code. **[5 points]**\n",
    "\n",
    "`Paste your implementation of class AdversarialLayer here. Include everything that you think is relevant.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c8d11c",
   "metadata": {},
   "source": [
    "[1] (b) We can also slowly ramp up the gradients from the adversarial network for better optimization. Implement a gradient reversal layer, with the following strategy:\n",
    "GRL(`grad`) = -$c\\ \\times$ `grad`,\n",
    "$c = \\frac{2}{1 + e^{\\frac{-\\alpha i}{\\max_i}}} - 1$.  Here $i$ is the iteration number and $\\max_i$ is the maximum number of iterations. Take $\\alpha = 10$. **[3 points]**\n",
    "\n",
    "`Expand your previous implementation of class AdversarialLayer here, with an added option for the ramp-up strategy.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3606d04",
   "metadata": {},
   "source": [
    "[1] (c) Why do you think the ramp-up strategy would help with the optimization? **[3 points]**\n",
    "\n",
    "`Answer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0371e75b",
   "metadata": {},
   "source": [
    "[2] (a) **Implement CDAN architecture**. We provide the class defintion for Domain adversarial neural network (DANN) as `discriminatorDANN` in `classification/network.py`. Inspired by this, implement a Conditional Domain Adversarial Network (CDAN) as class `discriminatorCDAN` in the same file. The network architecture of DANN and CDAN should be kept the same. The only difference between DANN and CDAN is input to the forward function of CDAN. Implement the multilinear conditioning $f \\bigotimes g$ defined in equation 4 in [1] and feed the result into the forward pass of the discriminator. Look out for \"IMPLEMENT YOUR CODE HERE\" in the code. **[5 points]**\n",
    "\n",
    "`Paste your code for forward function of class discriminatorCDAN here.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833e03d4",
   "metadata": {},
   "source": [
    "[2] (b) Briefly explain the advantages of using CDAN over DANN. **[3 points]**\n",
    "\n",
    "`Answer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9a7ac2",
   "metadata": {},
   "source": [
    "[3] (a) **Performance comparison**. Run the training script provided in `train.py` and report the adaptation accuracy in a tabular format. We provide numbers for source-only and target-only supervised models. Report the classification accuracy of DANN and CDAN on three transfer tasks, 1) $A \\to D$, 2) $D \\to W$ and, 3) $A \\to D$ on `office-31` dataset for unsupervised domain adaptation. (A - Amazon, D - DSLR and W - Webcam). Please correctly specify the dataset directory `data_dir` and other arguments when running train.py, e.g. `python train.py --data_dir /datasets/cs252d-sp22-a00-public/hw4_data/office31`. Check `opts.py` for input arguments of `train.py`. The accuracy reported in the table is the classification accuracy over all the classes in the target domain. We have provided the evaluation script in `eval.py`. You can simply specify the arguments and run it to get the required accuracy computed. **[15 points]**\n",
    "\n",
    "| Method | A -> W| D -> W | A -> D |\n",
    "|- | - | - | - |\n",
    "| Source Only         |   95.60                |      57.36             |        62.05           |\n",
    "| DANN                |   xx                |          xx         |        xx           |\n",
    "| CDAN                |       xx            |        xx           |        xx           |\n",
    "| Target Supervised   |     99.25              |       99.25            |       99.5            |\n",
    "\n",
    "[3] (b) What are your observations on the final table? Please briefly explain it. **[5 points]**\n",
    "\n",
    "`Answer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0d9d08",
   "metadata": {},
   "source": [
    "[4] (a) **tSNE visualization**. Train DANN and CDAN on task $A \\to D$ of `office-31`. Use your implementation of GRL and CDAN with a resnet-50 backbone (--resnet=50). Plot the tSNE visualization of the feature embeddings $f$ extracted by CDAN and DANN on A and D, before and after adaptation. To extract the feature embeddings for case of before adaptation, we provide weights for a pre-trained model in `/datasets/cs252d-sp22-a00-public/hw4_data/SourceOnly_amazon2dslr/best_model.pth.tar`.  (A - Amazon, D - DSLR and W - Webcam). Specifically, you need to plot three figures: (1) Before adaptation (trained on source domain A only) (2) DANN adaptation. (3) CDAN adaptation. Each figure should visualize feature embeddings on both of the source domain A (in blue) and target domain D (in red) respectively. See Fig. 3 in [1] for a reference. **[6 points]**\n",
    "\n",
    "`Paste your plots here.`\n",
    "\n",
    "[4] (b) What are your observations on the plots? Please briefly explain it. **[3 points]**\n",
    "\n",
    "`Answer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fa7955",
   "metadata": {},
   "source": [
    "### Section 2 - Segmentation\n",
    "[5] **Implement Adversarial Loss**. You have to implement the multi-level adversarial loss used by Tsai el al. in [2] (Refer to equations 3,4,5). The resnet-based segmentation network generates feature maps at two different resolutions. For each of the resolution, implement the segmentation loss and the adversarial loss. Check the scaffold provided in `segmentation/train_gta2cityscapesmulti.py`. Train the model in the Vanilla-Gan setup (args.gan = 'Vanilla') and use the evaluation script in `evaluate_cityscapes.py`. The datasets are in `/datasets/cs252d-sp22-a00-public/hw4_data`. Look out for \"IMPLEMENT THIS\" in the train loop. **[12 points]**\n",
    "\n",
    "`Plot each component of the loss curve (i.e. segmentation loss and adversarial loss at each resolution) here and report the accuacy.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614e11f0",
   "metadata": {},
   "source": [
    "### References\n",
    "1. [Conditional Adversarial Domain Adaptation](https://arxiv.org/pdf/1705.10667.pdf)\n",
    "2. [Learning to Adapt Structured Output Space for Semantic Segmentation](https://arxiv.org/pdf/1802.10349.pdf)\n",
    "3. [Unsupervised Domain Adaptation by Backpropagation](https://arxiv.org/abs/1409.7495)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
